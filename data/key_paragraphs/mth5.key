CUDA stands for Compute Unified Device Architecture and is a new hardware and software architecture for using NVIDIA Graphics Processing Units (GPUs) as a data-parallel computing device. CUDA is a parallel programming model [12,13] consists of a sequential host program, running on CPU host, and a kernel program, running on parallel GPU device. The host program sets up the data and transfers it to and from the GPU while the kernel program processes the data using a potentially large number of parallel threads. The threads of a kernel are grouped into a grid of thread blocks. The threads of a given block share a local store and may synchronize via barriers. Threads in different thread blocks cannot be synchronized. A modern NVIDIA GPU is built around an array of shared memory multiprocessors. Each multiprocessor is equipped with 8 scalar cores and 16 kB of high-bandwidth low-latency memory. The CUDA programming guide [13] provides tips for maximizing performance.
CPU Intel Xeon Quad-Core 2.66 GHz, 12 GB RAM (using gFortran), 
GPU NVIDIA Tesla T10, 240-core, 4 GB RAM (using CUDA).
