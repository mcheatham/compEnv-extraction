Our FEM models were generated on a virtual shared memory machine: 20 computers (Dell R720xd, each with dual Intel Xeon E5-2670 CPUs and DDR3 384 GB memory) are connected with InfiniBand FDR and shared virtually by vSMP Foundation [30]. For the generation of the 27 BlnDOF, 6.7 Bln-element FEM model used in Section III, most of the cost was spent on the generation of the FEMmodelc, conversion of FEMmodelc to FEMmodel, and division into FEMmodeli. Generation of FEMmodelc took 3 h 43 min with 160 OpenMP threads and 698 GB computer memory, conversion of FEMmodelc to FEMmodel took 4 h 53 min with 1,455 GB computer memory, and division into FEMmodeli with 82,944 divisions took 13 h 59 min with 1,708 GB computer memory. Because recent increases in computer memory have largely resolved the problem of the large computer memory required, only the construction time remains an issue. Almost half of the construction time is due to file I/O while the other half consists of meshing, calling the METIS library, and mapping between FEMmodel and FEMmodelc. The throughput of the file system used (NL-SAS, 7200 rpm HDD) is 100-200MB/s using the HDF5 library, while the total file size of the models is 1,649 GB. It is expected that construction time could be reduced by using faster file systems with parallel I/O. However, because we conduct many non-linear seismic wave amplification simulations using one FEM model to estimate the response against many earthquake scenarios, this model construction time is in fact a relatively minor problem. Indeed, construction of a FEM model with less load imbalance is much more desirable, even if the construction time is relatively long.
In this study, we measured the performance of GAMERA on the K computer, a massively parallel supercomputer at RIKEN, Advanced Institute for Computational Science [15]. The K computer consists of 82,944 compute nodes, each with single SPARC64TM VIIIfx CPUs with 6 MB L2 shared cache. Each of the eight cores of the SPARC CPU has two sets of twin fused multiply-and-add (FMA) units, which leads to four multiplications and four additions in one clock cycle. The number of operations per clock cycle is the same for single and double precision floats. The operating clock speed of the CPU is 2.0 GHz, leading to a peak performance of 8 FLOP × 2 GHz × 8 CPU cores = 128 GFLOPS per CPU. Each node has 16 GB of DDR3-SDRAM memory, with peak memory bandwidth of 64 GB/s. Tofu, a six-dimensional interconnection network, is used for communication between nodes [31]. Each node can communicate in four directions simultaneously, with 5 GB/s throughput in each direction. An OpenMPI 1.4.3 [32]-based MPI library optimized for the Tofu network was used, following the MPI 2.1 standard [33].
Finally, we checked the potential of GAMERA for use in an environment favorable for single-precision arithmetics. The floating-point performance of the Intel Xeon E5-2680 CPU for single-precision floats is twice that for double-precision floats, and this was expected to lead to a large change in time-to-solution. Fig. 7 shows the elapsed time and performance of GAMERA and GAMERA (DP,MG). Here, we use 256 compute nodes of Stampede at Texas Advanced Computing Center, The University of Texas at Austin [35], and 256 compute nodes of the K computer. Although each compute node of Stampede has an Intel Xeon Phi Coprocessor, we only used the dual Xeon E5-2680 CPUs for measurement. We set the DOF per compute node to 369 K DOF, which is similar to that of the weak scaling models in Table I. Then, 16 OpenMP threads were used per compute node of Stampede, and eight OpenMP threads were used per compute node of the K computer. From the figure, we can see that both the inner fine and inner coarse loops used in the preconditioner were accelerated when using mixed-precision arithmetics for both of the compute systems. The acceleration due to the use of mixed-precision arithmetics was about 1.27 times for the K computer with SPARC CPUs and 1.44 times for Stampede with Intel CPUs. Thus, we can expect further speed-up of GAMERA when using hardware with faster single-precision operation capabilities.
